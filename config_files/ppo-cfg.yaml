folders:
  parent_dir: "final"
  model_name: "sf3_sequential_with_finetuning"

policy_kwargs:
  #net_arch: [{ pi: [64, 64], vf: [32, 32] }]
  #net_arch: [64, 64]

ppo_settings:
  policy_type: "CnnPolicy"
  gamma: 0.99
  model_checkpoint: "0"
  train_lr: [ 2.5e-4, 2.5e-6 ]
  finetune_lr: [ 5.0e-5, 2.5e-6 ]
  train_cr: [ 0.15, 0.025 ]
  finetune_cr: [ 0.075, 0.025 ]
  batch_size: 1024 #8 #nminibatches gave different batch size depending on the number of environments: batch_size = (n_steps * n_envs) // nminibatches
  n_epochs: 10
  n_steps: 128
  autosave_freq: 500_000
  eval_freq: 500_000
  time_steps: 2_500_000
  n_eval_episodes: 25
  seeds: !!python/tuple [ 0 ]

  gae_lambda: 0.95
  ent_coef: 0.
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: false
  sde_sample_freq: -1
  normalize_advantage: true
  stats_window_size: 100 # Window size for the rollout logging, specifying the number of episodes to average the reported success rate, mean episode length, and mean reward over
  target_kl: null

imitation_settings:
  max_train_epochs: 5
  n_imitation_steps: 100_000
  