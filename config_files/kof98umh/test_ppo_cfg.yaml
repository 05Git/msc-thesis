folders:
  parent_dir: "./results/"
  model_name: "test_kof98umh"

settings:
  game_id: "umk3"
  step_ratio: 6
  frame_shape: !!python/tuple [128, 128, 1]
  continue_game: 0.
  action_space: "multi_discrete"
  characters: None
  difficulty: 4
  outfits: 1
#  fighting_style: 1
#  ultimate_style: !!python/tuple [1, 1, 1]

wrappers_settings:
  normalize_reward: true
  no_attack_buttons_combinations: false
  stack_frames: 4
  dilation: 1
  add_last_action: true
  stack_actions: 1
  scale: true
  exclude_image_scaling: true
  process_discrete_binary: true
  role_relative: true
  flatten: true
  filter_keys: []

policy_kwargs:
  #net_arch: [{ pi: [64, 64], vf: [32, 32] }]
  #net_arch: [64, 64]

ppo_settings:
  gamma: 0.94
  model_checkpoint: "0"
  learning_rate: [2e-4, 2e-6] # To start
  clip_range: [0.15, 0.025] # To start
  clip_range_vf: null
  batch_size: 1024 #8 #nminibatches gave different batch size depending on the number of environments: batch_size = (n_steps * n_envs) // nminibatches
  n_epochs: 4
  gae_lambda: 0.95
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: false
  sde_sample_freq: -1
  rollout_buffer_class: null # Rollout buffer class to use. If None, it will be automatically selected.
  rollout_buffer_kwargs: null # Keyword arguments to pass to the rollout buffer on creation.
  normalize_advantage: false # Whether to normalize or not the advantage
  stats_window_size: 100 # Window size for the rollout logging, specifying the number of episodes to average the reported success rate, mean episode length, and mean reward over
  target_kl: null
  n_steps: 256
  autosave_freq: 100_000
  time_steps: 500_000
  seed: 0
